{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445},{"sourceId":3388761,"sourceType":"datasetVersion","datasetId":2043034}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Topic_Modeling_Bertopic_opt\n\nOptimizador del modelo BERTopic para el modelado de temas","metadata":{}},{"cell_type":"code","source":"!pip install bertopic","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T00:42:52.529217Z","iopub.execute_input":"2025-07-13T00:42:52.529883Z","iopub.status.idle":"2025-07-13T00:44:07.688724Z","shell.execute_reply.started":"2025-07-13T00:42:52.529859Z","shell.execute_reply":"2025-07-13T00:44:07.687930Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -qU langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T23:01:57.033631Z","iopub.execute_input":"2025-06-30T23:01:57.033859Z","iopub.status.idle":"2025-06-30T23:02:05.897799Z","shell.execute_reply.started":"2025-06-30T23:01:57.033838Z","shell.execute_reply":"2025-06-30T23:02:05.897116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom bertopic import BERTopic\nfrom hdbscan import HDBSCAN\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom umap import UMAP\n\nfrom gensim.models import CoherenceModel\nimport gensim\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom gensim.matutils import corpus2csc\n\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T00:45:02.219878Z","iopub.execute_input":"2025-07-13T00:45:02.220469Z","iopub.status.idle":"2025-07-13T00:46:44.960593Z","shell.execute_reply.started":"2025-07-13T00:45:02.220443Z","shell.execute_reply":"2025-07-13T00:46:44.960009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_for_bertopic(text):\n    \"\"\"\n    Lightly clean tweets for BERTopic (BERT-based models).\n    Preserves stopwords and sentence structure for semantic understanding.\n\n    Removes: HTML tags, URLs, mentions, hashtag symbols.\n    \"\"\"\n    import re\n\n    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", '', text)  # Remove mentions\n    text = re.sub(r\"#\", '', text)  # Remove hashtag symbol only\n    return text.strip().lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T00:51:29.149184Z","iopub.execute_input":"2025-07-13T00:51:29.149458Z","iopub.status.idle":"2025-07-13T00:51:29.154378Z","shell.execute_reply.started":"2025-07-13T00:51:29.149437Z","shell.execute_reply":"2025-07-13T00:51:29.153505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/input/fake-or-real-news/fake_or_real_news.csv\")\ndf1=df1['title']+\"\\n\"+df1['text']\n\ndf2 = pd.read_csv(\"/kaggle/input/sentiment-analysis-dataset/test.csv\", encoding='ISO-8859-1')\ndf2 = df2['text']\n\ndf3 = pd.read_csv(\"/kaggle/input/sentiment-analysis-dataset/train.csv\", encoding='ISO-8859-1')\ndf3 = df3['text']\n\ndf = pd.concat([df1,df2,df3], ignore_index=True)\n\ndf = df.fillna('').astype(str)\ndf = df.astype(str).apply(clean_for_bertopic)\n\n#df = df['CleanText_BERT']\n\n\ndocs_subset = df.sample(frac=0.5, random_state=42).to_list()  # 20% del total\n\ndocs = df.to_list()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T00:54:26.593055Z","iopub.execute_input":"2025-07-13T00:54:26.593342Z","iopub.status.idle":"2025-07-13T00:54:27.828020Z","shell.execute_reply.started":"2025-07-13T00:54:26.593323Z","shell.execute_reply":"2025-07-13T00:54:27.827195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nimport numpy as np\nimport torch\n\ndef semantic_coherence(topic_model, model_name='all-mpnet-base-v2', top_n=10):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = SentenceTransformer(model_name).to(device)\n    \n    scores = []\n    for topic_id in topic_model.get_topic_info()[\"Topic\"]:\n        if topic_id == -1:\n            continue\n        topic_words = [word for word, _ in topic_model.get_topic(topic_id)[:top_n]]\n        embeddings = model.encode(topic_words, convert_to_tensor=True, device=device)\n        sim_matrix = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()\n        upper_triangle = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n        if len(upper_triangle) > 0:\n            scores.append(np.mean(upper_triangle))\n    \n    return np.mean(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T00:50:52.959634Z","iopub.execute_input":"2025-07-13T00:50:52.960000Z","iopub.status.idle":"2025-07-13T00:50:52.967377Z","shell.execute_reply.started":"2025-07-13T00:50:52.959955Z","shell.execute_reply":"2025-07-13T00:50:52.966611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom gensim.models import CoherenceModel\nfrom gensim.corpora import Dictionary\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom umap import UMAP\nfrom hdbscan import HDBSCAN\nfrom bertopic import BERTopic\n\ndef coherence_score(topic_model, docs_tokenized, dictionary):\n    topics = [\n        [w for w, _ in topic_model.get_topics()[tid]]\n        for tid in topic_model.get_topic_info()[\"Topic\"]\n        if tid != -1 and tid in topic_model.get_topics()\n    ]\n    cm = CoherenceModel(\n        topics=topics,\n        texts=docs_tokenized,\n        dictionary=dictionary,\n        coherence=\"c_v\"\n    )\n    return cm.get_coherence()\n\ndef objective(trial):\n    n_nb  = trial.suggest_int(\"n_neighbors\", 5, 45)\n    nc    = trial.suggest_int(\"n_components\", 5, 30)\n    mcs   = trial.suggest_int(\"min_cluster_size\", 5, 35)\n    msamp = trial.suggest_int(\"min_samples\", 5, mcs)\n    embedding = trial.suggest_categorical(\"embedding\", [\"all-mpnet-base-v2\", \"all-MiniLM-L6-v2\", \"paraphrase-MiniLM-L3-v2\"])\n\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\n\n    umap = UMAP(n_neighbors=n_nb, n_components=nc)\n    hdb  = HDBSCAN(min_cluster_size=mcs, min_samples=msamp, prediction_data=True)\n\n    topic_model = BERTopic(\n        vectorizer_model=vectorizer_model,\n        umap_model=umap,\n        hdbscan_model=hdb,\n        embedding_model=embedding,\n        verbose=False,\n        language='english'\n    )\n\n    topics, _ = topic_model.fit_transform(docs_subset)\n\n    tokenized_docs_subset = [doc.split() for doc in docs_subset]\n    dictionary_subset = Dictionary(tokenized_docs_subset)\n\n    n_topics = len(set(topic for topic in topics if topic != -1))\n    print(n_topics)\n    coherence = semantic_coherence(topic_model)\n    \n    return  coherence, n_topics\n\nstudy = optuna.create_study(directions=[\"maximize\", \"maximize\"])\nstudy.optimize(objective, n_trials=1000)\n\nfor trial in study.trials:\n    print(trial.values)  # muestra los dos valores optimizados","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T01:21:35.553148Z","iopub.execute_input":"2025-07-13T01:21:35.553437Z","iopub.status.idle":"2025-07-13T02:33:23.621249Z","shell.execute_reply.started":"2025-07-13T01:21:35.553415Z","shell.execute_reply":"2025-07-13T02:33:23.619302Z"}},"outputs":[],"execution_count":null}]}