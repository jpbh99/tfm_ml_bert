{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bert_sentiment_analysis\n\nNotebook para entrenamiento del modelo BERT para el analisis de sentimientos","metadata":{}},{"cell_type":"code","source":"!pip install pandas numpy scikit-learn transformers torch imblearn tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:03:01.888840Z","iopub.execute_input":"2025-06-19T19:03:01.889307Z","iopub.status.idle":"2025-06-19T19:04:27.333206Z","shell.execute_reply.started":"2025-06-19T19:03:01.889287Z","shell.execute_reply":"2025-06-19T19:04:27.332342Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:04:40.801952Z","iopub.execute_input":"2025-06-19T19:04:40.802996Z","iopub.status.idle":"2025-06-19T19:05:07.290964Z","shell.execute_reply.started":"2025-06-19T19:04:40.802947Z","shell.execute_reply":"2025-06-19T19:05:07.290240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset and inspect basic information\ntest_df = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/test.csv', encoding='ISO-8859-1')  \ndf = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')  \n\nprint(test_df['sentiment'].value_counts())\n\nprint(test_df.shape[0])\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:05:12.268475Z","iopub.execute_input":"2025-06-19T19:05:12.269088Z","iopub.status.idle":"2025-06-19T19:05:12.499014Z","shell.execute_reply.started":"2025-06-19T19:05:12.269062Z","shell.execute_reply":"2025-06-19T19:05:12.498391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n    import re\n\n    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # Remove URLs\n    text = re.sub(r\"@\\w+\", '', text)  # Remove mentions\n    text = re.sub(r\"#\", '', text)  # Remove hashtag symbol only\n    return text.strip().lower()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['text'] = df['text'].fillna('')\ndf['text'] = df['text'].astype(str).apply(clean_text)\ndf = df.dropna(subset=['sentiment'])\ntest_df = test_df.dropna(subset=['sentiment'])\ntest_df['text'] = test_df['text'].fillna('')\ntest_df['text'] = test_df['text'].astype(str).apply(clean_text)\n\ntrain_label_encoder = LabelEncoder()\ndf['sentiment_label'] = train_label_encoder.fit_transform(df['sentiment'])\nprint(f'Sentiment categories: {train_label_encoder.classes_}')\n\ntest_label_encoder = LabelEncoder()\ntest_df['sentiment_label'] = test_label_encoder.fit_transform(test_df['sentiment'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:05:16.740274Z","iopub.execute_input":"2025-06-19T19:05:16.740558Z","iopub.status.idle":"2025-06-19T19:05:16.779952Z","shell.execute_reply.started":"2025-06-19T19:05:16.740539Z","shell.execute_reply":"2025-06-19T19:05:16.778900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:05:19.640265Z","iopub.execute_input":"2025-06-19T19:05:19.640568Z","iopub.status.idle":"2025-06-19T19:05:23.203253Z","shell.execute_reply.started":"2025-06-19T19:05:19.640547Z","shell.execute_reply":"2025-06-19T19:05:23.202407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:06:24.542207Z","iopub.execute_input":"2025-06-19T19:06:24.542495Z","iopub.status.idle":"2025-06-19T19:06:24.549165Z","shell.execute_reply.started":"2025-06-19T19:06:24.542475Z","shell.execute_reply":"2025-06-19T19:06:24.548287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Dataset(df['text'].tolist(), df['sentiment_label'].tolist(), tokenizer)\nval_dataset = Dataset(test_df['text'].tolist(), test_df['sentiment_label'].tolist(), tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\nval_loader = DataLoader(val_dataset, batch_size = 32, shuffle = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:06:27.182365Z","iopub.execute_input":"2025-06-19T19:06:27.182960Z","iopub.status.idle":"2025-06-19T19:06:27.189971Z","shell.execute_reply.started":"2025-06-19T19:06:27.182935Z","shell.execute_reply":"2025-06-19T19:06:27.189237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-5)\n\nepochs = 5\ntrain_loss_list, val_loss_list = [],[]\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device), labels=batch['labels'].to(device))\n\n        train_loss += outputs.loss.item()\n        outputs.loss.backward()\n        optimizer.step()          \n        optimizer.zero_grad() \n\n    print(f\"Epoch {epoch+1} Train Loss: {train_loss/len(train_loader)}\")\n    train_loss_list.append(train_loss/len(train_loader))\n\n    model.eval()\n    val_loss = 0\n    preds, true_pred = [],[]\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"): \n            outputs = model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device), labels=batch['labels'].to(device))\n            val_loss += outputs.loss.item()\n\n            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n            true_pred.extend(batch['labels'].numpy())\n    print(f\"Epoch {epoch+1} Val Loss: {val_loss/len(val_loader)}\")\n    val_loss_list.append(val_loss/len(val_loader))\n    print(classification_report(true_pred, preds, target_names=test_label_encoder.classes_))\n            \n\n\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:06:32.021190Z","iopub.execute_input":"2025-06-19T19:06:32.021542Z","iopub.status.idle":"2025-06-19T19:34:51.681508Z","shell.execute_reply.started":"2025-06-19T19:06:32.021492Z","shell.execute_reply":"2025-06-19T19:34:51.680256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Graficar\nplt.plot(range(1,epochs+1), train_loss_list, label='Training Loss')\nplt.plot(range(1,epochs+1), val_loss_list, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training vs Validation Loss')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:35:22.662017Z","iopub.execute_input":"2025-06-19T19:35:22.662955Z","iopub.status.idle":"2025-06-19T19:35:22.951587Z","shell.execute_reply.started":"2025-06-19T19:35:22.662927Z","shell.execute_reply":"2025-06-19T19:35:22.950759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"id2label = {0: \"Negativo\", 1: \"Neutro\", 2: \"Positivo\"}\n\ndef pred_sent(texto):\n    # Tokenizar la entrada\n    inputs = tokenizer(\n            texto,\n            add_special_tokens=True,\n            max_length=128,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n    # Mover los tensores al dispositivo (CPU o GPU)\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n\n    # Desactivar gradientes para inferencia\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        pred_id = torch.argmax(logits, dim=1).item()\n\n    #return id2label[pred_id]\n    return pred_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:35:27.820699Z","iopub.execute_input":"2025-06-19T19:35:27.821305Z","iopub.status.idle":"2025-06-19T19:35:27.827152Z","shell.execute_reply.started":"2025-06-19T19:35:27.821281Z","shell.execute_reply":"2025-06-19T19:35:27.826186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#texto = \"I'm so incredibly angry with how things have turned out. I gave everything I had — my time, my energy, my trust — and for what? To be ignored, dismissed, and treated like I never mattered. People keep crossing the line, thinking I’ll stay quiet forever, but I’m done pretending it’s okay. I’m tired of the fake apologies, the empty promises, and the constant disrespect. I’ve kept my mouth shut for far too long, but not anymore. I deserve better than this, and I won’t tolerate being walked all over just to keep the peace. If no one’s going to take me seriously, then they’re about to see what happens when I stop holding back.\"\n#sentimiento = predecir_sentimiento(texto)\n#print(f\"Sentimiento: {sentimiento}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T12:08:21.551847Z","iopub.execute_input":"2025-06-02T12:08:21.552575Z","iopub.status.idle":"2025-06-02T12:08:21.575981Z","shell.execute_reply.started":"2025-06-02T12:08:21.552551Z","shell.execute_reply":"2025-06-02T12:08:21.575445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sent_list = []\nfor i in range(test_df.shape[0]):\n    text = str(test_df['text'][i])\n    sent = pred_sent(text)\n    sent_list.append(sent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:35:31.499997Z","iopub.execute_input":"2025-06-19T19:35:31.500782Z","iopub.status.idle":"2025-06-19T19:36:05.331708Z","shell.execute_reply.started":"2025-06-19T19:35:31.500749Z","shell.execute_reply":"2025-06-19T19:36:05.331004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(test_df['sentiment_label'], sent_list)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=train_label_encoder.classes_)\ndisp.plot()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:36:10.168427Z","iopub.execute_input":"2025-06-19T19:36:10.168850Z","iopub.status.idle":"2025-06-19T19:36:10.437422Z","shell.execute_reply.started":"2025-06-19T19:36:10.168817Z","shell.execute_reply":"2025-06-19T19:36:10.436459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/bert_sentiment_model\", exist_ok=True)\nos.makedirs(\"/kaggle/working/bert_sentiment_model/model\", exist_ok=True)\nos.makedirs(\"/kaggle/working/bert_sentiment_model/tokenizer\", exist_ok=True)\n\n# Save model\nmodel.save_pretrained('/kaggle/working/bert_sentiment_model/model/bert_sentiment_model', save_embedding_model=False)\ntokenizer.save_pretrained('/kaggle/working/bert_sentiment_model/tokenizer/bert_tokenizer')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T19:36:14.516130Z","iopub.execute_input":"2025-06-19T19:36:14.516465Z","iopub.status.idle":"2025-06-19T19:36:15.554033Z","shell.execute_reply.started":"2025-06-19T19:36:14.516441Z","shell.execute_reply":"2025-06-19T19:36:15.553366Z"}},"outputs":[],"execution_count":null}]}