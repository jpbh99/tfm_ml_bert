{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimizador de Hiperparametros\n\nOptimizador de los modelos de ML para el analisis de sentimientos","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n\nimport optuna\nfrom gensim.models import CoherenceModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:23:50.842343Z","iopub.execute_input":"2025-07-10T13:23:50.842567Z","iopub.status.idle":"2025-07-10T13:24:52.553270Z","shell.execute_reply.started":"2025-07-10T13:23:50.842547Z","shell.execute_reply":"2025-07-10T13:24:52.552272Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load the dataset and inspect basic information\ntest_df = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/test.csv', encoding='ISO-8859-1')  \ndf = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:24:52.555276Z","iopub.execute_input":"2025-07-10T13:24:52.555815Z","iopub.status.idle":"2025-07-10T13:24:52.782023Z","shell.execute_reply.started":"2025-07-10T13:24:52.555790Z","shell.execute_reply":"2025-07-10T13:24:52.781212Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Número total de instancias\nnum_instancias = len(df)\n\n# Conteo de patrones por clase\nconteo_clases = df['sentiment'].value_counts()\n\n# Proporción (frecuencia relativa) por clase\nproporcion_clases = df['sentiment'].value_counts(normalize=True)\n\n# Mostrar resultados\nprint(\"Número total de instancias:\", num_instancias)\nprint(\"\\nConteo por clase:\\n\", conteo_clases)\nprint(\"\\nProporción por clase:\\n\", proporcion_clases)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:26:21.511811Z","iopub.execute_input":"2025-07-10T13:26:21.512232Z","iopub.status.idle":"2025-07-10T13:26:21.536457Z","shell.execute_reply.started":"2025-07-10T13:26:21.512201Z","shell.execute_reply":"2025-07-10T13:26:21.535253Z"}},"outputs":[{"name":"stdout","text":"Número total de instancias: 27481\n\nConteo por clase:\n sentiment\nneutral     11118\npositive     8582\nnegative     7781\nName: count, dtype: int64\n\nProporción por clase:\n sentiment\nneutral     0.404570\npositive    0.312288\nnegative    0.283141\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Número total de instancias\nnum_instancias = len(test_df)\n\n# Conteo de patrones por clase\nconteo_clases = test_df['sentiment'].value_counts()\n\n# Proporción (frecuencia relativa) por clase\nproporcion_clases = test_df['sentiment'].value_counts(normalize=True)\n\n# Mostrar resultados\nprint(\"Número total de instancias:\", num_instancias)\nprint(\"\\nConteo por clase:\\n\", conteo_clases)\nprint(\"\\nProporción por clase:\\n\", proporcion_clases)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T13:27:03.642996Z","iopub.execute_input":"2025-07-10T13:27:03.643422Z","iopub.status.idle":"2025-07-10T13:27:03.654139Z","shell.execute_reply.started":"2025-07-10T13:27:03.643393Z","shell.execute_reply":"2025-07-10T13:27:03.652943Z"}},"outputs":[{"name":"stdout","text":"Número total de instancias: 4815\n\nConteo por clase:\n sentiment\nneutral     1430\npositive    1103\nnegative    1001\nName: count, dtype: int64\n\nProporción por clase:\n sentiment\nneutral     0.404641\npositive    0.312111\nnegative    0.283248\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df['text'] = df['text'].fillna('')\ndf = df.dropna(subset=['sentiment'])\ntest_df = test_df.dropna(subset=['sentiment'])\ntest_df['text'] = test_df['text'].fillna('')\n\n\ntrain_label_encoder = LabelEncoder()\ndf['sentiment_label'] = train_label_encoder.fit_transform(df['sentiment'])\n#df = df.sample(frac=0.2, random_state=42)  # 20% del total\n\ntest_label_encoder = LabelEncoder()\ntest_df['sentiment_label'] = test_label_encoder.fit_transform(test_df['sentiment'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:14:28.342359Z","iopub.execute_input":"2025-06-17T16:14:28.342672Z","iopub.status.idle":"2025-06-17T16:14:28.374740Z","shell.execute_reply.started":"2025-06-17T16:14:28.342648Z","shell.execute_reply":"2025-06-17T16:14:28.373790Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def objective(trial):\n    C = trial.suggest_float(\"C\", 0.8, 5.0, log=True)\n    kernel = trial.suggest_categorical(\"kernel\", [\"rbf\"]) #[\"linear\", \"rbf\", \"poly\"]\n    gamma = trial.suggest_categorical(\"gamma\", [\"scale\"]) #trial.suggest_float(\"gamma\", 1e-5, 1.0, log=True) if kernel != \"linear\" else \"scale\"\n    #degree = trial.suggest_int(\"degree\", 2, 5) if kernel == \"poly\" else 3\n\n    svm_pipeline = Pipeline([\n    (\"tfidf\", TfidfVectorizer()),\n    (\"svc\", SVC(C=C, kernel=kernel , gamma=gamma))\n])\n    svm_pipeline.fit(df['text'], df['sentiment_label'])\n    score = svm_pipeline.score(test_df['text'], test_df['sentiment_label'])\n\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\nprint(study.best_params, study.best_value)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T15:25:34.153494Z","iopub.execute_input":"2025-06-15T15:25:34.153830Z","iopub.status.idle":"2025-06-15T21:32:36.858537Z","shell.execute_reply.started":"2025-06-15T15:25:34.153806Z","shell.execute_reply":"2025-06-15T21:32:36.856914Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"[I 2025-06-15 15:25:34,157] A new study created in memory with name: no-name-e73150e9-74fc-492e-b946-435e7348ae65\n[I 2025-06-15 15:33:57,167] Trial 0 finished with value: 0.7136389360498019 and parameters: {'C': 4.4569155779335174, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 0 with value: 0.7136389360498019.\n[I 2025-06-15 15:42:05,502] Trial 1 finished with value: 0.7147707979626485 and parameters: {'C': 2.6274165891211854, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.7147707979626485.\n[I 2025-06-15 15:50:22,944] Trial 2 finished with value: 0.7136389360498019 and parameters: {'C': 3.219840415417644, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 1 with value: 0.7147707979626485.\n[I 2025-06-15 15:58:29,199] Trial 3 finished with value: 0.7150537634408602 and parameters: {'C': 2.537412886420518, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 3 with value: 0.7150537634408602.\n[I 2025-06-15 16:06:43,110] Trial 4 finished with value: 0.7147707979626485 and parameters: {'C': 2.896574482485424, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 3 with value: 0.7150537634408602.\n[I 2025-06-15 16:14:18,823] Trial 5 finished with value: 0.7192982456140351 and parameters: {'C': 1.901710396437338, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 16:22:27,231] Trial 6 finished with value: 0.7150537634408602 and parameters: {'C': 2.5721980786399725, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 16:30:53,664] Trial 7 finished with value: 0.7139219015280136 and parameters: {'C': 3.4792845926641998, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 16:39:09,188] Trial 8 finished with value: 0.7144878324844369 and parameters: {'C': 2.833410243680998, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 16:47:35,493] Trial 9 finished with value: 0.7136389360498019 and parameters: {'C': 3.6660907238123692, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 16:53:51,943] Trial 10 finished with value: 0.7176004527447651 and parameters: {'C': 1.2597006801705628, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 17:06:17,051] Trial 12 finished with value: 0.7170345217883418 and parameters: {'C': 1.3712304723855875, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 17:11:32,029] Trial 13 finished with value: 0.7037351443123939 and parameters: {'C': 0.8145650094334549, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 5 with value: 0.7192982456140351.\n[I 2025-06-15 17:18:48,918] Trial 14 finished with value: 0.72014714204867 and parameters: {'C': 1.659166092564281, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 14 with value: 0.72014714204867.\n[I 2025-06-15 17:26:16,427] Trial 15 finished with value: 0.7190152801358234 and parameters: {'C': 1.7735821014679298, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 14 with value: 0.72014714204867.\n[I 2025-06-15 17:33:53,027] Trial 16 finished with value: 0.7178834182229767 and parameters: {'C': 1.8426973954117936, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 14 with value: 0.72014714204867.\n[I 2025-06-15 17:40:59,937] Trial 17 finished with value: 0.7198641765704584 and parameters: {'C': 1.5686138409023085, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 14 with value: 0.72014714204867.\n[I 2025-06-15 17:47:54,985] Trial 18 finished with value: 0.7187323146576118 and parameters: {'C': 1.4932318177806405, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 14 with value: 0.72014714204867.\n[I 2025-06-15 17:53:43,956] Trial 19 finished with value: 0.7164685908319185 and parameters: {'C': 0.9770402516027028, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 14 with value: 0.72014714204867.\n[I 2025-06-15 18:01:32,465] Trial 20 finished with value: 0.7184493491794001 and parameters: {'C': 2.124454572223605, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 14 with value: 0.72014714204867.\n[I 2025-06-15 18:08:45,019] Trial 21 finished with value: 0.7207130730050934 and parameters: {'C': 1.6096091371852976, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 18:15:49,687] Trial 22 finished with value: 0.7195812110922467 and parameters: {'C': 1.559131954082071, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 18:23:48,994] Trial 23 finished with value: 0.7184493491794001 and parameters: {'C': 2.1321490407049795, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 18:29:42,058] Trial 24 finished with value: 0.7159026598754952 and parameters: {'C': 1.0807345692649037, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 18:36:57,825] Trial 25 finished with value: 0.7204301075268817 and parameters: {'C': 1.6086839504426629, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 18:45:04,692] Trial 26 finished with value: 0.7190152801358234 and parameters: {'C': 2.2079361328100453, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 18:52:25,667] Trial 27 finished with value: 0.7198641765704584 and parameters: {'C': 1.6680737153055953, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 18:58:42,039] Trial 28 finished with value: 0.7176004527447651 and parameters: {'C': 1.333166488825486, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:07:13,496] Trial 29 finished with value: 0.7139219015280136 and parameters: {'C': 4.664388007373114, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:13:16,793] Trial 30 finished with value: 0.7164685908319185 and parameters: {'C': 0.9745622549473271, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:20:20,421] Trial 31 finished with value: 0.7190152801358234 and parameters: {'C': 1.5251961233078786, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:27:48,762] Trial 32 finished with value: 0.7198641765704584 and parameters: {'C': 1.6729787477565943, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:34:33,146] Trial 33 finished with value: 0.7187323146576118 and parameters: {'C': 1.4100749190130253, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:42:20,745] Trial 34 finished with value: 0.7195812110922467 and parameters: {'C': 1.9697478077355894, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:50:27,313] Trial 35 finished with value: 0.7184493491794001 and parameters: {'C': 2.2763360071791423, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 19:56:34,458] Trial 36 finished with value: 0.7181663837011885 and parameters: {'C': 1.2693281764806472, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:04:01,920] Trial 37 finished with value: 0.7198641765704584 and parameters: {'C': 1.7211763105062055, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:11:30,639] Trial 38 finished with value: 0.7198641765704584 and parameters: {'C': 1.575557604745228, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:19:40,888] Trial 39 finished with value: 0.7150537634408602 and parameters: {'C': 2.4457287681009547, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:27:27,982] Trial 40 finished with value: 0.7195812110922467 and parameters: {'C': 1.9620124329888378, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:34:45,812] Trial 41 finished with value: 0.7198641765704584 and parameters: {'C': 1.5852638659976874, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:42:17,786] Trial 42 finished with value: 0.7195812110922467 and parameters: {'C': 1.7361906186224625, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:49:04,237] Trial 43 finished with value: 0.7184493491794001 and parameters: {'C': 1.4320920019350534, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 20:54:40,700] Trial 44 finished with value: 0.7187323146576118 and parameters: {'C': 1.1629016969397974, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 21:02:29,036] Trial 45 finished with value: 0.7190152801358234 and parameters: {'C': 1.8947225123570006, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 21:08:41,275] Trial 46 finished with value: 0.7178834182229767 and parameters: {'C': 1.264874802105237, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 21:17:05,547] Trial 47 finished with value: 0.712790039615167 and parameters: {'C': 3.976242096991382, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 21:25:19,996] Trial 48 finished with value: 0.7144878324844369 and parameters: {'C': 2.926565654605822, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n[I 2025-06-15 21:32:36,849] Trial 49 finished with value: 0.7204301075268817 and parameters: {'C': 1.6616131047138416, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 21 with value: 0.7207130730050934.\n","output_type":"stream"},{"name":"stdout","text":"{'C': 1.6096091371852976, 'kernel': 'rbf', 'gamma': 'scale'} 0.7207130730050934\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def objective(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 120)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\"])\n\n    model = RandomForestClassifier(\n        n_estimators=n_estimators,\n        min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        max_features=max_features,\n        random_state=42,\n    )\n\n    rf_pipeline = Pipeline([\n    (\"tfidf\", TfidfVectorizer()),\n    (\"rf\", model)\n    ])\n    rf_pipeline.fit(df['text'], df['sentiment_label'])\n    score = rf_pipeline.score(test_df['text'], test_df['sentiment_label'])\n\n    return score\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params, study.best_value) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T16:27:01.533714Z","iopub.execute_input":"2025-06-17T16:27:01.534070Z","iopub.status.idle":"2025-06-17T17:16:17.438272Z","shell.execute_reply.started":"2025-06-17T16:27:01.534044Z","shell.execute_reply":"2025-06-17T17:16:17.437264Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"[I 2025-06-17 16:27:01,537] A new study created in memory with name: no-name-3f0a15c3-e978-4e06-8b7c-0aa8577ba1b3\n[I 2025-06-17 16:27:09,792] Trial 0 finished with value: 0.6910016977928692 and parameters: {'n_estimators': 115, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.6910016977928692.\n[I 2025-06-17 16:27:58,688] Trial 1 finished with value: 0.7028862478777589 and parameters: {'n_estimators': 117, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:28:05,639] Trial 2 finished with value: 0.688737973967176 and parameters: {'n_estimators': 117, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:29:37,384] Trial 3 finished with value: 0.6901528013582343 and parameters: {'n_estimators': 113, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:29:42,924] Trial 4 finished with value: 0.6794001131861913 and parameters: {'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:29:47,163] Trial 5 finished with value: 0.6745897000565931 and parameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:29:54,715] Trial 6 finished with value: 0.6907187323146576 and parameters: {'n_estimators': 105, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:29:58,837] Trial 7 finished with value: 0.6644029428409735 and parameters: {'n_estimators': 118, 'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:30:08,443] Trial 8 finished with value: 0.6941143180531976 and parameters: {'n_estimators': 104, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:30:19,535] Trial 9 finished with value: 0.6977928692699491 and parameters: {'n_estimators': 112, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:30:23,928] Trial 10 finished with value: 0.6731748726655348 and parameters: {'n_estimators': 108, 'min_samples_split': 10, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:31:19,958] Trial 11 finished with value: 0.6949632144878325 and parameters: {'n_estimators': 112, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:31:26,468] Trial 12 finished with value: 0.688737973967176 and parameters: {'n_estimators': 109, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:32:16,885] Trial 13 finished with value: 0.6941143180531976 and parameters: {'n_estimators': 114, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:32:23,125] Trial 14 finished with value: 0.6867572156196944 and parameters: {'n_estimators': 117, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:32:33,430] Trial 15 finished with value: 0.6935483870967742 and parameters: {'n_estimators': 111, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:32:42,151] Trial 16 finished with value: 0.6955291454442558 and parameters: {'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:32:52,542] Trial 17 finished with value: 0.6983588002263724 and parameters: {'n_estimators': 116, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:32:56,537] Trial 18 finished with value: 0.6632710809281268 and parameters: {'n_estimators': 115, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:33:02,706] Trial 19 finished with value: 0.6876061120543294 and parameters: {'n_estimators': 118, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:33:09,577] Trial 20 finished with value: 0.6884550084889643 and parameters: {'n_estimators': 115, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:33:20,097] Trial 21 finished with value: 0.6955291454442558 and parameters: {'n_estimators': 110, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:33:30,108] Trial 22 finished with value: 0.6924165251839276 and parameters: {'n_estimators': 107, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:34:38,459] Trial 23 finished with value: 0.6943972835314092 and parameters: {'n_estimators': 113, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:34:46,726] Trial 24 finished with value: 0.6943972835314092 and parameters: {'n_estimators': 116, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:34:56,687] Trial 25 finished with value: 0.6960950764006791 and parameters: {'n_estimators': 112, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:35:56,659] Trial 26 finished with value: 0.6938313525749858 and parameters: {'n_estimators': 119, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:36:03,698] Trial 27 finished with value: 0.687889077532541 and parameters: {'n_estimators': 116, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:36:09,019] Trial 28 finished with value: 0.677136389360498 and parameters: {'n_estimators': 114, 'min_samples_split': 9, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:36:17,028] Trial 29 finished with value: 0.6921335597057159 and parameters: {'n_estimators': 111, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:36:21,746] Trial 30 finished with value: 0.6737408036219581 and parameters: {'n_estimators': 116, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:36:31,737] Trial 31 finished with value: 0.6960950764006791 and parameters: {'n_estimators': 112, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:36:42,191] Trial 32 finished with value: 0.6969439728353141 and parameters: {'n_estimators': 114, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:37:30,588] Trial 33 finished with value: 0.7028862478777589 and parameters: {'n_estimators': 114, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:38:20,308] Trial 34 finished with value: 0.7028862478777589 and parameters: {'n_estimators': 117, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:39:10,970] Trial 35 finished with value: 0.7017543859649122 and parameters: {'n_estimators': 118, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:40:04,109] Trial 36 finished with value: 0.6958121109224674 and parameters: {'n_estimators': 118, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7028862478777589.\n[I 2025-06-17 16:40:55,342] Trial 37 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:41:46,300] Trial 38 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:41:54,934] Trial 39 finished with value: 0.6924165251839276 and parameters: {'n_estimators': 120, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:42:43,478] Trial 40 finished with value: 0.6992076966610073 and parameters: {'n_estimators': 119, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:43:33,648] Trial 41 finished with value: 0.702037351443124 and parameters: {'n_estimators': 119, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:44:26,162] Trial 42 finished with value: 0.6941143180531976 and parameters: {'n_estimators': 117, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:45:20,436] Trial 43 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:46:10,092] Trial 44 finished with value: 0.6975099037917374 and parameters: {'n_estimators': 120, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:46:18,390] Trial 45 finished with value: 0.6938313525749858 and parameters: {'n_estimators': 119, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:46:23,032] Trial 46 finished with value: 0.6686474250141483 and parameters: {'n_estimators': 120, 'min_samples_split': 8, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:46:34,012] Trial 47 finished with value: 0.6958121109224674 and parameters: {'n_estimators': 118, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:46:43,334] Trial 48 finished with value: 0.6966610073571025 and parameters: {'n_estimators': 103, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:47:33,917] Trial 49 finished with value: 0.702037351443124 and parameters: {'n_estimators': 119, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:47:42,702] Trial 50 finished with value: 0.6924165251839276 and parameters: {'n_estimators': 120, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:48:32,250] Trial 51 finished with value: 0.7028862478777589 and parameters: {'n_estimators': 117, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:49:21,899] Trial 52 finished with value: 0.7028862478777589 and parameters: {'n_estimators': 117, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:49:32,527] Trial 53 finished with value: 0.6946802490096208 and parameters: {'n_estimators': 118, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:50:18,909] Trial 54 finished with value: 0.700339558573854 and parameters: {'n_estimators': 115, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:50:29,408] Trial 55 finished with value: 0.6932654216185625 and parameters: {'n_estimators': 117, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:51:17,340] Trial 56 finished with value: 0.702037351443124 and parameters: {'n_estimators': 115, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:51:27,727] Trial 57 finished with value: 0.6969439728353141 and parameters: {'n_estimators': 119, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:52:17,930] Trial 58 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:52:30,307] Trial 59 finished with value: 0.6963780418788907 and parameters: {'n_estimators': 120, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:52:35,042] Trial 60 finished with value: 0.6777023203169213 and parameters: {'n_estimators': 100, 'min_samples_split': 8, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:53:24,503] Trial 61 finished with value: 0.702037351443124 and parameters: {'n_estimators': 119, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:54:13,299] Trial 62 finished with value: 0.7017543859649122 and parameters: {'n_estimators': 118, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:55:03,730] Trial 63 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:55:14,447] Trial 64 finished with value: 0.6983588002263724 and parameters: {'n_estimators': 120, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:55:25,139] Trial 65 finished with value: 0.6983588002263724 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:55:30,266] Trial 66 finished with value: 0.6774193548387096 and parameters: {'n_estimators': 119, 'min_samples_split': 8, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:56:16,543] Trial 67 finished with value: 0.7011884550084889 and parameters: {'n_estimators': 116, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:56:24,984] Trial 68 finished with value: 0.6921335597057159 and parameters: {'n_estimators': 118, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:56:30,896] Trial 69 finished with value: 0.6859083191850595 and parameters: {'n_estimators': 113, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:57:23,167] Trial 70 finished with value: 0.6943972835314092 and parameters: {'n_estimators': 119, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:58:13,290] Trial 71 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:59:03,322] Trial 72 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 16:59:53,663] Trial 73 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:00:44,515] Trial 74 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:00:55,231] Trial 75 finished with value: 0.6983588002263724 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:01:43,992] Trial 76 finished with value: 0.6992076966610073 and parameters: {'n_estimators': 119, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:02:36,524] Trial 77 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:02:46,125] Trial 78 finished with value: 0.6972269383135258 and parameters: {'n_estimators': 107, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:03:38,410] Trial 79 finished with value: 0.6958121109224674 and parameters: {'n_estimators': 118, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:03:49,480] Trial 80 finished with value: 0.6960950764006791 and parameters: {'n_estimators': 119, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:04:40,028] Trial 81 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:05:29,896] Trial 82 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:06:20,157] Trial 83 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:07:10,194] Trial 84 finished with value: 0.702037351443124 and parameters: {'n_estimators': 119, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:07:20,773] Trial 85 finished with value: 0.6946802490096208 and parameters: {'n_estimators': 118, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:08:11,045] Trial 86 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:08:58,328] Trial 87 finished with value: 0.6992076966610073 and parameters: {'n_estimators': 119, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:09:08,678] Trial 88 finished with value: 0.6958121109224674 and parameters: {'n_estimators': 118, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:09:58,045] Trial 89 finished with value: 0.702037351443124 and parameters: {'n_estimators': 119, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:10:50,857] Trial 90 finished with value: 0.6946802490096208 and parameters: {'n_estimators': 120, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:11:33,363] Trial 91 finished with value: 0.7009054895302773 and parameters: {'n_estimators': 101, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:12:23,385] Trial 92 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:13:13,647] Trial 93 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:13:24,046] Trial 94 finished with value: 0.6969439728353141 and parameters: {'n_estimators': 119, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:14:17,665] Trial 95 finished with value: 0.7034521788341822 and parameters: {'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:15:10,337] Trial 96 finished with value: 0.6941143180531976 and parameters: {'n_estimators': 117, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:15:20,779] Trial 97 finished with value: 0.6969439728353141 and parameters: {'n_estimators': 119, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:16:10,161] Trial 98 finished with value: 0.7017543859649122 and parameters: {'n_estimators': 118, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n[I 2025-06-17 17:16:17,433] Trial 99 finished with value: 0.689586870401811 and parameters: {'n_estimators': 120, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 37 with value: 0.7034521788341822.\n","output_type":"stream"},{"name":"stdout","text":"{'n_estimators': 120, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'sqrt'} 0.7034521788341822\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def objective(trial):\n    C = trial.suggest_float(\"C\", 1e-4, 10.0, log=True)\n    penalty = trial.suggest_categorical(\"penalty\", [\"l2\", \"none\"])\n    solver = trial.suggest_categorical(\"solver\", [\"lbfgs\", \"saga\", \"newton-cg\"])  # compatibles con L2 o none\n    max_iter = trial.suggest_int(\"max_iter\", 100, 1000)\n\n    model = LogisticRegression(\n        C=C,\n        penalty=penalty,\n        solver=solver,\n        max_iter=max_iter,\n        random_state=42,\n    )\n\n    lr_pipeline = Pipeline([\n    (\"tfidf\", TfidfVectorizer()),\n    (\"lr\", model)\n    ])\n    lr_pipeline.fit(df['text'], df['sentiment_label'])\n    score = lr_pipeline.score(test_df['text'], test_df['sentiment_label'])\n    return score\n    \nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params, study.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T17:29:21.501901Z","iopub.execute_input":"2025-06-17T17:29:21.502206Z","iopub.status.idle":"2025-06-17T17:44:27.428056Z","shell.execute_reply.started":"2025-06-17T17:29:21.502185Z","shell.execute_reply":"2025-06-17T17:44:27.426524Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"[I 2025-06-17 17:29:21,505] A new study created in memory with name: no-name-02f30239-3c8a-452f-adef-9a058f5e6c22\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n[I 2025-06-17 17:29:30,210] Trial 0 finished with value: 0.6148839841539332 and parameters: {'C': 0.0017235671711406438, 'penalty': 'none', 'solver': 'saga', 'max_iter': 348}. Best is trial 0 with value: 0.6148839841539332.\n[I 2025-06-17 17:29:30,886] Trial 1 finished with value: 0.4538766270514997 and parameters: {'C': 0.004205101930842154, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 242}. Best is trial 0 with value: 0.6148839841539332.\n[I 2025-06-17 17:29:31,904] Trial 2 finished with value: 0.5826259196378042 and parameters: {'C': 0.02161583292570402, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 439}. Best is trial 0 with value: 0.6148839841539332.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n[I 2025-06-17 17:29:58,027] Trial 3 finished with value: 0.5681946802490097 and parameters: {'C': 0.0038803201660533204, 'penalty': 'none', 'solver': 'lbfgs', 'max_iter': 822}. Best is trial 0 with value: 0.6148839841539332.\n[I 2025-06-17 17:29:59,344] Trial 4 finished with value: 0.40633842671194115 and parameters: {'C': 0.0010861050379425091, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 239}. Best is trial 0 with value: 0.6148839841539332.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n[I 2025-06-17 17:30:22,247] Trial 5 finished with value: 0.5721561969439728 and parameters: {'C': 0.14601367581822428, 'penalty': 'none', 'solver': 'lbfgs', 'max_iter': 704}. Best is trial 0 with value: 0.6148839841539332.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n[I 2025-06-17 17:30:43,321] Trial 6 finished with value: 0.5973401245048104 and parameters: {'C': 0.036258841167586556, 'penalty': 'none', 'solver': 'saga', 'max_iter': 857}. Best is trial 0 with value: 0.6148839841539332.\n[I 2025-06-17 17:30:44,625] Trial 7 finished with value: 0.7023203169213356 and parameters: {'C': 0.5339915663460846, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 394}. Best is trial 7 with value: 0.7023203169213356.\n[I 2025-06-17 17:30:45,609] Trial 8 finished with value: 0.6397849462365591 and parameters: {'C': 0.0637139969075465, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 148}. Best is trial 7 with value: 0.7023203169213356.\n[I 2025-06-17 17:30:46,593] Trial 9 finished with value: 0.5172608941709111 and parameters: {'C': 0.009496428564683709, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 391}. Best is trial 7 with value: 0.7023203169213356.\n[I 2025-06-17 17:30:48,695] Trial 10 finished with value: 0.6907187323146576 and parameters: {'C': 4.224212005812787, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 624}. Best is trial 7 with value: 0.7023203169213356.\n[I 2025-06-17 17:30:50,681] Trial 11 finished with value: 0.6907187323146576 and parameters: {'C': 4.442211474198762, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 625}. Best is trial 7 with value: 0.7023203169213356.\n[I 2025-06-17 17:30:52,780] Trial 12 finished with value: 0.6924165251839276 and parameters: {'C': 4.098244844214801, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 536}. Best is trial 7 with value: 0.7023203169213356.\n[I 2025-06-17 17:30:54,273] Trial 13 finished with value: 0.7071307300509337 and parameters: {'C': 0.7996324268023888, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 496}. Best is trial 13 with value: 0.7071307300509337.\n[I 2025-06-17 17:30:55,665] Trial 14 finished with value: 0.6980758347481607 and parameters: {'C': 0.4302812796861017, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 495}. Best is trial 13 with value: 0.7071307300509337.\n[I 2025-06-17 17:30:56,387] Trial 15 finished with value: 0.4046406338426712 and parameters: {'C': 0.00010004002880208645, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 348}. Best is trial 13 with value: 0.7071307300509337.\n[I 2025-06-17 17:30:57,782] Trial 16 finished with value: 0.7045840407470289 and parameters: {'C': 0.6173473692993966, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 989}. Best is trial 13 with value: 0.7071307300509337.\n[I 2025-06-17 17:31:03,172] Trial 17 finished with value: 0.7071307300509337 and parameters: {'C': 0.8018379266527058, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 965}. Best is trial 13 with value: 0.7071307300509337.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n[I 2025-06-17 17:31:34,200] Trial 18 finished with value: 0.5664968873797397 and parameters: {'C': 1.2229438481894912, 'penalty': 'none', 'solver': 'lbfgs', 'max_iter': 979}. Best is trial 13 with value: 0.7071307300509337.\n[I 2025-06-17 17:31:37,329] Trial 19 finished with value: 0.6745897000565931 and parameters: {'C': 0.13438291380960843, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 781}. Best is trial 13 with value: 0.7071307300509337.\n[I 2025-06-17 17:31:50,782] Trial 20 finished with value: 0.6833616298811545 and parameters: {'C': 8.512843292678284, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 621}. Best is trial 13 with value: 0.7071307300509337.\n[I 2025-06-17 17:31:56,931] Trial 21 finished with value: 0.7088285229202037 and parameters: {'C': 0.9083369221372963, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 998}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:32:04,926] Trial 22 finished with value: 0.7059988681380871 and parameters: {'C': 1.9119905645394517, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 918}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:32:08,463] Trial 23 finished with value: 0.6827956989247311 and parameters: {'C': 0.1831174444919287, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 738}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:32:16,564] Trial 24 finished with value: 0.7088285229202037 and parameters: {'C': 1.5587456180301316, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 898}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:32:25,506] Trial 25 finished with value: 0.7059988681380871 and parameters: {'C': 1.9072406837221891, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 873}. Best is trial 21 with value: 0.7088285229202037.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n[I 2025-06-17 17:32:47,921] Trial 26 finished with value: 0.5707413695529145 and parameters: {'C': 0.23278062889302742, 'penalty': 'none', 'solver': 'lbfgs', 'max_iter': 699}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:32:49,985] Trial 27 finished with value: 0.6403508771929824 and parameters: {'C': 0.06446759662801238, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 933}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:33:04,048] Trial 28 finished with value: 0.6844934917940011 and parameters: {'C': 9.349153979835998, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 792}. Best is trial 21 with value: 0.7088285229202037.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n[I 2025-06-17 17:33:34,033] Trial 29 finished with value: 0.5690435766836446 and parameters: {'C': 1.4404491220576632, 'penalty': 'none', 'solver': 'lbfgs', 'max_iter': 901}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:33:35,088] Trial 30 finished with value: 0.688737973967176 and parameters: {'C': 0.24932442618754255, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 486}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:33:41,064] Trial 31 finished with value: 0.7088285229202037 and parameters: {'C': 0.9113971332285554, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 991}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:33:52,793] Trial 32 finished with value: 0.7000565930956423 and parameters: {'C': 2.7577640578851255, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 994}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:33:59,099] Trial 33 finished with value: 0.7074136955291455 and parameters: {'C': 1.0224457345857612, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 875}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:34:03,483] Trial 34 finished with value: 0.6932654216185625 and parameters: {'C': 0.3230113612305799, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 865}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:34:10,451] Trial 35 finished with value: 0.7088285229202037 and parameters: {'C': 1.0886760720084494, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 803}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:34:12,929] Trial 36 finished with value: 0.6598754951895869 and parameters: {'C': 0.09412206584059367, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 936}. Best is trial 21 with value: 0.7088285229202037.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n[I 2025-06-17 17:34:40,171] Trial 37 finished with value: 0.5693265421618563 and parameters: {'C': 0.01793439933457392, 'penalty': 'none', 'solver': 'lbfgs', 'max_iter': 819}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:34:49,867] Trial 38 finished with value: 0.6969439728353141 and parameters: {'C': 3.078493738133384, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 754}. Best is trial 21 with value: 0.7088285229202037.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n[I 2025-06-17 17:35:09,770] Trial 39 finished with value: 0.5973401245048104 and parameters: {'C': 0.36167529278359545, 'penalty': 'none', 'solver': 'saga', 'max_iter': 818}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:35:10,492] Trial 40 finished with value: 0.4046406338426712 and parameters: {'C': 0.0006300028522588947, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 684}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:35:15,707] Trial 41 finished with value: 0.7082625919637804 and parameters: {'C': 0.9308048317068084, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 883}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:35:23,961] Trial 42 finished with value: 0.7062818336162988 and parameters: {'C': 1.9563670690189818, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 940}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:35:31,276] Trial 43 finished with value: 0.7065647990945104 and parameters: {'C': 0.6371896191735233, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 843}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:35:44,563] Trial 44 finished with value: 0.6842105263157895 and parameters: {'C': 6.173857799182719, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 896}. Best is trial 21 with value: 0.7088285229202037.\n[I 2025-06-17 17:35:45,641] Trial 45 finished with value: 0.7096774193548387 and parameters: {'C': 1.2990598194518517, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 958}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:35:46,812] Trial 46 finished with value: 0.7006225240520656 and parameters: {'C': 2.6231741560481527, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 998}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:35:48,312] Trial 47 finished with value: 0.6867572156196944 and parameters: {'C': 5.279923145049548, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 952}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:35:49,307] Trial 48 finished with value: 0.6015846066779853 and parameters: {'C': 0.029681947770899608, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 247}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:35:50,384] Trial 49 finished with value: 0.6994906621392191 and parameters: {'C': 0.4738453219097127, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 919}. Best is trial 45 with value: 0.7096774193548387.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n[I 2025-06-17 17:36:13,576] Trial 50 finished with value: 0.5973401245048104 and parameters: {'C': 0.006805249794599485, 'penalty': 'none', 'solver': 'saga', 'max_iter': 962}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:20,896] Trial 51 finished with value: 0.7082625919637804 and parameters: {'C': 1.207292113484249, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 842}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:26,771] Trial 52 finished with value: 0.7082625919637804 and parameters: {'C': 0.9398722476533291, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 902}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:33,768] Trial 53 finished with value: 0.7045840407470289 and parameters: {'C': 0.6169759101136282, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 787}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:34,920] Trial 54 finished with value: 0.7085455574419921 and parameters: {'C': 1.5453988621075385, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 972}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:36,084] Trial 55 finished with value: 0.7074136955291455 and parameters: {'C': 1.6594940784918792, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 998}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:37,403] Trial 56 finished with value: 0.6943972835314092 and parameters: {'C': 3.6205121068535076, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 960}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:38,570] Trial 57 finished with value: 0.4207696661007357 and parameters: {'C': 0.002205554486035792, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 966}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:40,146] Trial 58 finished with value: 0.6842105263157895 and parameters: {'C': 6.124979766162493, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 923}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:41,198] Trial 59 finished with value: 0.6972269383135258 and parameters: {'C': 0.35834604905186085, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 845}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:42,232] Trial 60 finished with value: 0.6610073571024335 and parameters: {'C': 0.09917336678341147, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 900}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:48,041] Trial 61 finished with value: 0.7082625919637804 and parameters: {'C': 0.8825977041499167, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 872}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:49,842] Trial 62 finished with value: 0.7054329371816639 and parameters: {'C': 2.100326578526922, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 972}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:36:56,980] Trial 63 finished with value: 0.7085455574419921 and parameters: {'C': 1.17374166344027, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 660}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:02,114] Trial 64 finished with value: 0.7043010752688172 and parameters: {'C': 0.562814128526265, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 657}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:11,319] Trial 65 finished with value: 0.7088285229202037 and parameters: {'C': 1.5057322095276173, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 585}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:19,444] Trial 66 finished with value: 0.7091114883984154 and parameters: {'C': 1.544799239559589, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 734}. Best is trial 45 with value: 0.7096774193548387.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n[I 2025-06-17 17:37:37,388] Trial 67 finished with value: 0.5758347481607244 and parameters: {'C': 3.703687653012273, 'penalty': 'none', 'solver': 'lbfgs', 'max_iter': 538}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:41,920] Trial 68 finished with value: 0.6873231465761177 and parameters: {'C': 0.22376174770657067, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 565}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:51,089] Trial 69 finished with value: 0.7034521788341822 and parameters: {'C': 2.4750165836489533, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 750}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:56,345] Trial 70 finished with value: 0.7048670062252406 and parameters: {'C': 0.6652827831421125, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 582}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:58,082] Trial 71 finished with value: 0.7088285229202037 and parameters: {'C': 1.5053640917880278, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 719}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:37:59,674] Trial 72 finished with value: 0.7096774193548387 and parameters: {'C': 1.3968016290711964, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 718}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:38:01,076] Trial 73 finished with value: 0.6980758347481607 and parameters: {'C': 0.4106360214121792, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 768}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:38:02,893] Trial 74 finished with value: 0.6994906621392191 and parameters: {'C': 2.7145019737043627, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 824}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:38:04,467] Trial 75 finished with value: 0.7076966610073571 and parameters: {'C': 0.8089936944759082, 'penalty': 'l2', 'solver': 'newton-cg', 'max_iter': 730}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:38:20,297] Trial 76 finished with value: 0.6822297679683079 and parameters: {'C': 7.952495534768763, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 596}. Best is trial 45 with value: 0.7096774193548387.\n[I 2025-06-17 17:38:26,950] Trial 77 finished with value: 0.7085455574419921 and parameters: {'C': 1.2408341978684427, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 467}. Best is trial 45 with value: 0.7096774193548387.\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n  warnings.warn(\n[W 2025-06-17 17:44:27,375] Trial 78 failed with parameters: {'C': 4.666595869723847, 'penalty': 'none', 'solver': 'newton-cg', 'max_iter': 686} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/1610384148.py\", line 19, in objective\n    lr_pipeline.fit(df['text'], df['sentiment_label'])\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 63, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1985, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1913, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 468, in _logistic_regression_path\n    w0, n_iter_i = _newton_cg(\n                   ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/optimize.py\", line 193, in _newton_cg\n    xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/optimize.py\", line 88, in _cg\n    Ap = fhess_p(psupi)\n         ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_linear_loss.py\", line 646, in hessp\n    hess_prod[:, :n_features] = tmp.T @ X + l2_reg_strength * s\n                                ~~~~~~^~~\n  File \"/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\", line 738, in __rmatmul__\n    return self._rmatmul_dispatch(other)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\", line 719, in _rmatmul_dispatch\n    ret = self.transpose()._matmul_dispatch(tr)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\", line 624, in _matmul_dispatch\n    return self._matmul_multivector(other)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/scipy/sparse/_compressed.py\", line 539, in _matmul_multivector\n    fn(M, N, n_vecs, self.indptr, self.indices, self.data,\nKeyboardInterrupt\n[W 2025-06-17 17:44:27,377] Trial 78 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1610384148.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1610384148.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     ])\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlr_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1292\u001b[0m             path_func(\n\u001b[1;32m   1293\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1983\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1911\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0ml2_reg_strength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg_strength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             w0, n_iter_i = _newton_cg(\n\u001b[0m\u001b[1;32m    469\u001b[0m                 \u001b[0mhess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36m_newton_cg\u001b[0;34m(grad_hess, func, grad, x0, args, tol, maxiter, maxinner, line_search, warn)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Inner loop: solve the Newton update by conjugate gradient, to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# avoid inverting the Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mxsupi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfhess_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtermcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0malphak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36m_cg\u001b[0;34m(fhess_p, fgrad, maxiter, tol)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mAp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfhess_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;31m# check curvature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mcurv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsupi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mhessp\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0;31m# function is run after that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mhess_prod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 \u001b[0mhess_prod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_reg_strength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                     \u001b[0mhess_prod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m__rmatmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    736\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    737\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 738\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rmatmul_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;31m####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m_rmatmul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m_matmul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    622\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matmul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36m_matmul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# csr_matvecs or csc_matvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvecs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         fn(M, N, n_vecs, self.indptr, self.indices, self.data,\n\u001b[0m\u001b[1;32m    540\u001b[0m            other.ravel(), result.ravel())\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"def objective(trial):\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 3, 50)\n    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n    algorithm = trial.suggest_categorical(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"])\n    p = trial.suggest_int(\"p\", 1, 2)  # 1: manhattan, 2: euclidean\n    leaf_size = trial.suggest_int(\"leaf_size\", 10, 100)\n\n    model = KNeighborsClassifier(\n        n_neighbors=n_neighbors,\n        weights=weights,\n        algorithm=algorithm,\n        p=p,\n        leaf_size=leaf_size\n    )\n\n    knn_pipeline = Pipeline([\n    (\"tfidf\", TfidfVectorizer()),\n    (\"knn\", model)\n    ])\n    knn_pipeline.fit(df['text'], df['sentiment_label'])\n    score = knn_pipeline.score(test_df['text'], test_df['sentiment_label'])\n    return score\n    \nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params, study.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T17:45:13.062522Z","iopub.execute_input":"2025-06-17T17:45:13.062812Z","iopub.status.idle":"2025-06-17T18:06:54.037195Z","shell.execute_reply.started":"2025-06-17T17:45:13.062790Z","shell.execute_reply":"2025-06-17T18:06:54.036345Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"[I 2025-06-17 17:45:13,066] A new study created in memory with name: no-name-123cb054-32da-45a1-9e4a-1f7ba643009b\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:45:26,708] Trial 0 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 42, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 40}. Best is trial 0 with value: 0.6179966044142614.\n[I 2025-06-17 17:45:40,476] Trial 1 finished with value: 0.5874363327674024 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 35}. Best is trial 0 with value: 0.6179966044142614.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:45:46,259] Trial 2 finished with value: 0.42105263157894735 and parameters: {'n_neighbors': 45, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 1, 'leaf_size': 58}. Best is trial 0 with value: 0.6179966044142614.\n[I 2025-06-17 17:45:59,897] Trial 3 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 41}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:46:13,647] Trial 4 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 46, 'weights': 'distance', 'algorithm': 'brute', 'p': 2, 'leaf_size': 43}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:46:19,551] Trial 5 finished with value: 0.41765704584040747 and parameters: {'n_neighbors': 50, 'weights': 'uniform', 'algorithm': 'brute', 'p': 1, 'leaf_size': 16}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:46:25,472] Trial 6 finished with value: 0.4235993208828523 and parameters: {'n_neighbors': 27, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 1, 'leaf_size': 90}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:46:31,352] Trial 7 finished with value: 0.41765704584040747 and parameters: {'n_neighbors': 50, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 1, 'leaf_size': 69}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:46:37,115] Trial 8 finished with value: 0.4366157328805886 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'algorithm': 'brute', 'p': 1, 'leaf_size': 37}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:46:51,383] Trial 9 finished with value: 0.6174306734578382 and parameters: {'n_neighbors': 48, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2, 'leaf_size': 48}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:47:05,072] Trial 10 finished with value: 0.6151669496321449 and parameters: {'n_neighbors': 32, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 13}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:47:18,909] Trial 11 finished with value: 0.6154499151103565 and parameters: {'n_neighbors': 37, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 27}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:47:32,599] Trial 12 finished with value: 0.6165817770232032 and parameters: {'n_neighbors': 38, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 56}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:47:46,280] Trial 13 finished with value: 0.6154499151103565 and parameters: {'n_neighbors': 37, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 68}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:48:00,444] Trial 14 finished with value: 0.6035653650254669 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 27}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:48:14,171] Trial 15 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 41, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 88}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:48:27,974] Trial 16 finished with value: 0.61092246745897 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 99}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:48:41,827] Trial 17 finished with value: 0.46576117713638937 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 79}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:48:55,735] Trial 18 finished with value: 0.6177136389360498 and parameters: {'n_neighbors': 41, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2, 'leaf_size': 81}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:49:09,422] Trial 19 finished with value: 0.6015846066779853 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 68}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:49:15,210] Trial 20 finished with value: 0.42105263157894735 and parameters: {'n_neighbors': 33, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1, 'leaf_size': 100}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:49:29,030] Trial 21 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 42, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 49}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:49:42,796] Trial 22 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 42, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 26}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:49:56,496] Trial 23 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 45, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 39}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:50:10,406] Trial 24 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 45, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 62}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:50:24,211] Trial 25 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 61}. Best is trial 3 with value: 0.619128466327108.\n[I 2025-06-17 17:50:38,504] Trial 26 finished with value: 0.6174306734578382 and parameters: {'n_neighbors': 45, 'weights': 'uniform', 'algorithm': 'brute', 'p': 2, 'leaf_size': 46}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:50:52,239] Trial 27 finished with value: 0.6185625353706847 and parameters: {'n_neighbors': 35, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 2, 'leaf_size': 35}. Best is trial 3 with value: 0.619128466327108.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:51:05,992] Trial 28 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 52}. Best is trial 28 with value: 0.6202603282399547.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:51:19,808] Trial 29 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 52}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:51:33,496] Trial 30 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 53}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:51:47,314] Trial 31 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 51}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:52:01,030] Trial 32 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 52}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:52:15,151] Trial 33 finished with value: 0.6157328805885682 and parameters: {'n_neighbors': 40, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 54}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:52:28,844] Trial 34 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 64}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:52:42,558] Trial 35 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 45, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 44}. Best is trial 28 with value: 0.6202603282399547.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:52:56,414] Trial 36 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 2, 'leaf_size': 53}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:53:02,191] Trial 37 finished with value: 0.42246745897000565 and parameters: {'n_neighbors': 44, 'weights': 'distance', 'algorithm': 'brute', 'p': 1, 'leaf_size': 75}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:53:16,002] Trial 38 finished with value: 0.6174306734578382 and parameters: {'n_neighbors': 48, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2, 'leaf_size': 59}. Best is trial 28 with value: 0.6202603282399547.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:53:30,043] Trial 39 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 32}. Best is trial 28 with value: 0.6202603282399547.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:53:36,002] Trial 40 finished with value: 0.43208828522920206 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'algorithm': 'ball_tree', 'p': 1, 'leaf_size': 49}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:53:49,880] Trial 41 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 52}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:54:03,721] Trial 42 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 42}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:54:17,347] Trial 43 finished with value: 0.6177136389360498 and parameters: {'n_neighbors': 43, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 57}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:54:31,401] Trial 44 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 52}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:54:45,043] Trial 45 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 39, 'weights': 'distance', 'algorithm': 'brute', 'p': 2, 'leaf_size': 65}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:54:59,165] Trial 46 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 47}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:55:12,826] Trial 47 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 41}. Best is trial 28 with value: 0.6202603282399547.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:55:26,467] Trial 48 finished with value: 0.6177136389360498 and parameters: {'n_neighbors': 43, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 31}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:55:40,593] Trial 49 finished with value: 0.610073571024335 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2, 'leaf_size': 57}. Best is trial 28 with value: 0.6202603282399547.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:55:46,376] Trial 50 finished with value: 0.4207696661007357 and parameters: {'n_neighbors': 46, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 1, 'leaf_size': 73}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:56:00,112] Trial 51 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 64}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:56:13,808] Trial 52 finished with value: 0.6202603282399547 and parameters: {'n_neighbors': 50, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 50}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:56:27,451] Trial 53 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 60}. Best is trial 28 with value: 0.6202603282399547.\n[I 2025-06-17 17:56:41,502] Trial 54 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 45}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:56:55,143] Trial 55 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 44}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:57:08,955] Trial 56 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 46, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 2, 'leaf_size': 55}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:57:22,616] Trial 57 finished with value: 0.6177136389360498 and parameters: {'n_neighbors': 43, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 46}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:57:36,214] Trial 58 finished with value: 0.5812110922467459 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'algorithm': 'brute', 'p': 2, 'leaf_size': 51}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:57:50,483] Trial 59 finished with value: 0.6174306734578382 and parameters: {'n_neighbors': 48, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2, 'leaf_size': 38}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:58:04,130] Trial 60 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 41, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 22}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:58:18,238] Trial 61 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 56}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:58:31,887] Trial 62 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 46, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 58}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:58:45,715] Trial 63 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 54}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:58:59,373] Trial 64 finished with value: 0.6174306734578382 and parameters: {'n_neighbors': 44, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 47}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 17:59:13,019] Trial 65 finished with value: 0.523203169213356 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 55}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:59:26,844] Trial 66 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 67}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:59:40,498] Trial 67 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 62}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 17:59:54,600] Trial 68 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 36, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 69}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:00:08,432] Trial 69 finished with value: 0.6174306734578382 and parameters: {'n_neighbors': 44, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 71}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:00:22,315] Trial 70 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 79}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:00:36,008] Trial 71 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 86}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:00:49,700] Trial 72 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 46, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 88}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:01:03,886] Trial 73 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 81}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:01:17,575] Trial 74 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 82}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:01:31,345] Trial 75 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 93}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:01:45,048] Trial 76 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 46, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 93}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:01:58,865] Trial 77 finished with value: 0.6092246745897001 and parameters: {'n_neighbors': 30, 'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 95}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:02:12,605] Trial 78 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 82}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:02:26,364] Trial 79 finished with value: 0.6174306734578382 and parameters: {'n_neighbors': 44, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 77}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:02:40,230] Trial 80 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 85}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:02:53,861] Trial 81 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 82}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:03:07,929] Trial 82 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 85}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:03:21,586] Trial 83 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 94}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:03:35,254] Trial 84 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 45, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 90}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:03:49,061] Trial 85 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 78}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:04:02,741] Trial 86 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 84}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:04:16,797] Trial 87 finished with value: 0.6157328805885682 and parameters: {'n_neighbors': 40, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 2, 'leaf_size': 88}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:04:30,458] Trial 88 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 42, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 97}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 18:04:36,317] Trial 89 finished with value: 0.4202037351443124 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'brute', 'p': 1, 'leaf_size': 76}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:04:50,236] Trial 90 finished with value: 0.5959252971137521 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 74}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:05:03,896] Trial 91 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 85}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:05:17,617] Trial 92 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 45, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 79}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:05:31,223] Trial 93 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 91}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:05:44,860] Trial 94 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 80}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:05:58,993] Trial 95 finished with value: 0.6179966044142614 and parameters: {'n_neighbors': 46, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 87}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:06:12,635] Trial 96 finished with value: 0.619128466327108 and parameters: {'n_neighbors': 48, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 72}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:06:26,671] Trial 97 finished with value: 0.6211092246745897 and parameters: {'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'kd_tree', 'p': 2, 'leaf_size': 66}. Best is trial 54 with value: 0.6211092246745897.\n/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_base.py:557: UserWarning: cannot use tree with sparse input: using brute force\n  warnings.warn(\"cannot use tree with sparse input: using brute force\")\n[I 2025-06-17 18:06:40,307] Trial 98 finished with value: 0.6188455008488964 and parameters: {'n_neighbors': 47, 'weights': 'distance', 'algorithm': 'ball_tree', 'p': 2, 'leaf_size': 83}. Best is trial 54 with value: 0.6211092246745897.\n[I 2025-06-17 18:06:54,033] Trial 99 finished with value: 0.6177136389360498 and parameters: {'n_neighbors': 43, 'weights': 'distance', 'algorithm': 'brute', 'p': 2, 'leaf_size': 86}. Best is trial 54 with value: 0.6211092246745897.\n","output_type":"stream"},{"name":"stdout","text":"{'n_neighbors': 49, 'weights': 'distance', 'algorithm': 'auto', 'p': 2, 'leaf_size': 45} 0.6211092246745897\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def objective(trial):\n    alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n    fit_prior = trial.suggest_categorical(\"fit_prior\", [True, False])\n\n    model = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n    \n    nb_pipeline = Pipeline([\n    (\"tfidf\", TfidfVectorizer()),\n    (\"nb\", model)\n    ])\n    nb_pipeline.fit(df['text'], df['sentiment_label'])\n    score = nb_pipeline.score(test_df['text'], test_df['sentiment_label'])\n    return score\n    \nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params, study.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T19:16:37.393885Z","iopub.execute_input":"2025-06-17T19:16:37.394164Z","iopub.status.idle":"2025-06-17T19:16:37.476593Z","shell.execute_reply.started":"2025-06-17T19:16:37.394137Z","shell.execute_reply":"2025-06-17T19:16:37.475507Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2594799584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'fit_prior' is not defined"],"ename":"NameError","evalue":"name 'fit_prior' is not defined","output_type":"error"}],"execution_count":1}]}